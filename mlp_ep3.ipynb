{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lembrar de colocar nomes no cabeçalho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando libs e definindo sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(X):\n",
    "   return 1/(1+np.exp(-X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de treinamento da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_n_1_training(x, d, eta, Nt, Ne, Nn, W0N_1, W01_2):\n",
    "    \"\"\"\n",
    "    J_MSE, W_1, W1_2 = mlp_n_1_training(x, d, Nn, eta, Nt, Nb, Ne, W0)\n",
    "    Saídas:\n",
    "    J_MSE: valor da função custo ao longo das épocas\n",
    "    W0_1: vetor de pesos da camada 1. - Cada neurônio tem um peso somente (uma entrada) \n",
    "    W1_2: vetor de pesos do neurônio 1 da camada de saída\n",
    "    Entradas:\n",
    "    x: sinal de entrada: (apenas uma feature)\n",
    "    d: sinal desejado\n",
    "    eta: passo de adaptação\n",
    "    Nt: número de dados de treinamento\n",
    "    Ne: número de épocas\n",
    "    \"\"\"\n",
    "    WN_1  = W0N_1.reshape(2,Nn).copy()\n",
    "    W1_2 = W01_2.reshape(Nn+1,1).copy()\n",
    "\n",
    "    # inicialização do vetor que contém o valor da função custo\n",
    "    J_MSE = np.zeros((Ne, 1))\n",
    "\n",
    "    # Juntamos o vetor de entrada com o sinal desejado e inserimos\n",
    "    # uma coluna de uns para levar em conta o bias\n",
    "    Xd = np.hstack((np.ones((Nt, 1)), x.reshape(-1,1), d.reshape(-1,1)))\n",
    "\n",
    "    v1 = np.zeros((Nt,Nn))\n",
    "    y1 = np.zeros((Nt,Nn))\n",
    "\n",
    "    #For das épocas\n",
    "    for k in range(Ne):\n",
    "\n",
    "        np.random.shuffle(Xd)\n",
    "        X = Xd[:, 0:2]\n",
    "        d = Xd[:, [2]]\n",
    "\n",
    "        #Cálculo progressivo da camada 1\n",
    "        for neuron in range(Nn):\n",
    "            v1[:, neuron] = X@WN_1[:,neuron]\n",
    "            y1[:, neuron] = sigmoid(v1[:, neuron])\n",
    "\n",
    "        dphiN_1 = y1*(1 - y1) #dphis da camada 1\n",
    "\n",
    "        #Neurônio da camada de saída\n",
    "        X2 = np.hstack((np.ones((Nt, 1)), y1))\n",
    "        v1_2 = X2@W1_2\n",
    "        y1_2 = sigmoid(v1_2)\n",
    "        dphi1_2 = y1_2 * (1 - y1_2)\n",
    "\n",
    "        #Erro da última camada\n",
    "        e1_2 = d - y1_2\n",
    "\n",
    "        #Algoritmo de backpropagation\n",
    "        delta1_2 = dphi1_2*e1_2\n",
    "        delta1 = np.zeros((Nt,Nn))\n",
    "\n",
    "        for neuron in range(Nn):\n",
    "            delta1[:,neuron] = dphiN_1[:, neuron]*(delta1_2[:,0]*W1_2[neuron+1,0]) #Vetor de deltas dos neurônios da camada 1\n",
    "            \n",
    "        #Atualização dos pesos da última camada\n",
    "        W1_2 = W1_2 + eta*(delta1_2.T @ X2).T / Nt\n",
    "\n",
    "        #Atualização dos pesos da primeira camada\n",
    "        WN_1 = WN_1 + eta*(X.T @ delta1) / Nt\n",
    "        \n",
    "        J_MSE[k] = (np.linalg.norm(e1_2)) ** 2\n",
    "    \n",
    "    return J_MSE, WN_1, W1_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de teste da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_n_1_testing(x, d, WN_1, W1_2, Nn, Nteste):\n",
    "   \"\"\"\n",
    "   J_MSE,y = redeMLP_teste_21(x, d, W1_1, W2_1, W1_2, Nn, Nteste)\n",
    "   Saídas:\n",
    "   J_MSE: valor da função custo no teste\n",
    "   y: saída da rede MLP\n",
    "   Entradas:\n",
    "   x: sinal de entrada\n",
    "   d: sinal desejado\n",
    "   Nn: num de neurônios na camada oculta\n",
    "   WN_1 vetor de pesos da camada 1. - Cada neurônio tem um peso somente (uma entrada) \n",
    "   W1_2 vetor de pesos do neurônio 1 da camada de saída\n",
    "   Nteste: número de dados de teste\n",
    "   \"\"\"\n",
    "\n",
    "   # Adicionando bias\n",
    "   X_test = np.hstack((np.ones((Nteste, 1)), x.reshape(-1, 1)))\n",
    "  \n",
    "   # Output da primeira camada\n",
    "   v1_test = X_test @ WN_1\n",
    "   y1_test = sigmoid(v1_test)\n",
    "   \n",
    "   # Output da camada de saída\n",
    "   X2_test = np.hstack((np.ones((Nteste, 1)), y1_test))\n",
    "   v1_2_test = X2_test @ W1_2\n",
    "   y1_2_test = sigmoid(v1_2_test)\n",
    "  \n",
    "   # Calculando erro quadrático médio\n",
    "   mse = np.mean((y1_2_test - d) ** 2)\n",
    "   abse = np.sum(np.abs(y1_2_test - d))\n",
    "   \n",
    "   return y1_2_test, mse, abse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando o modelo com as funções propostas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando funções auxiliares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_training_test_set(function: str, num_points: int) -> Tuple:\n",
    "    if function == 'inverse':\n",
    "        x = np.random.uniform(1, 100, num_points)\n",
    "        y = (1/x)\n",
    "\n",
    "    if function == 'log':\n",
    "        x = np.random.uniform(1, 10, num_points)\n",
    "        y = np.log10(x)\n",
    "\n",
    "    if function == 'exp':\n",
    "        x = np.random.uniform(1, 10, num_points)\n",
    "        y = np.exp(-x)\n",
    "    \n",
    "    if function == 'sin':\n",
    "        x = np.random.uniform(0, np.pi/2, num_points)\n",
    "        y = np.sin(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função auxiliar para inicializar pesos com inicialização Xavier\n",
    "\n",
    "def xavier_init(fan_in, fan_out):\n",
    "   limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "   return np.random.uniform(-limit, limit, (fan_in, fan_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinar e testar o modelo\n",
    "\n",
    "def train_test_model(num_pts_train, num_pts_test, num_epochs, Nn_hid_layer, eta, init, function):\n",
    "   \n",
    "    if init == 'xavier':\n",
    "        # Definindo pesos da camada oculta\n",
    "        fan_in_1 = 2  # número de features (incluindo bias)\n",
    "        fan_out_1 = Nn_hid_layer\n",
    "        W0N_1 = xavier_init(fan_in_1, fan_out_1)\n",
    "\n",
    "        # Definindo pesos da camda de sáida\n",
    "        fan_in_2 = Nn_hid_layer + 1 \n",
    "        fan_out_2 = 1  # Número de neurônios de saída\n",
    "        W01_2 = xavier_init(fan_in_2, fan_out_2)\n",
    "    \n",
    "    else: #Inicialização uniforme\n",
    "        W0N_1 = 0.2 * np.random.rand(2, Nn_hid_layer) - 0.01\n",
    "        W01_2 = 0.2 * np.random.rand(Nn_hid_layer+1, 1) - 0.01\n",
    "    \n",
    "    #Gerando datasets\n",
    "    x_train, d_train = get_training_test_set(function, num_pts_train)\n",
    "    x_test, d_test = get_training_test_set(function, num_pts_test)\n",
    "\n",
    "    #Trainando o modelo\n",
    "    J_MSE_train, WN_1, W1_2 = mlp_n_1_training(x=x_train, d=d_train, eta=eta, Nt=num_pts_train, Ne=num_epochs, Nn=Nn_hid_layer, W0N_1=W0N_1, W01_2=W01_2)\n",
    "\n",
    "    #Testando o modelo\n",
    "    y_test, mse, abse =  mlp_n_1_testing(x=x_test, d=d_test, WN_1=WN_1, W1_2=W1_2, Nn=Nn_hid_layer, Nteste=num_pts_test)\n",
    "\n",
    "    return J_MSE_train, x_test, d_test, y_test, mse, abse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para construir gráfico do JMSE de treinamento e os gráficos das funções obtidas com o modelo\n",
    "\n",
    "def print_results(J_MSE_train, x_test, d_test, y_test, num_epochs, Nn_hid_layer, eta, function, mse, abse):  \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    # Construindo gráficos\n",
    "    ax1.plot(range(num_epochs), J_MSE_train, label='J_MSE_train')\n",
    "    title = \"JMSE | \" + \"Ne: \" + str(num_epochs) + \" Nn: \"  + str(Nn_hid_layer) + \" eta: \" + str(eta)\n",
    "    ax1.set_title(title)\n",
    "    ax1.set_ylabel('JMSE_train')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Gráfico comparativo da resposta dos dados de teste e valores retornados pelo modelo\n",
    "    ax2.plot(x_test, d_test, label='d (função esperada)', marker='.', linestyle='', alpha=0.5) \n",
    "    ax2.plot(x_test, y_test, label='y_test (função obtida)', marker='.', linestyle='', alpha=0.5)\n",
    "    ax2.set_title(f'Esperado vs resultado do teste\\n(mse = {round(mse,5)}, abse = {round(abse, 5)})')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylabel(function)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'./results/{function}/Ne{num_epochs}_Nn{Nn_hid_layer}_eta{eta}.png', dpi=300)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo parâmetros constantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo tamanho dos sets de treinamento e teste\n",
    "num_pts_train = 10000\n",
    "num_pts_test = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de uso da função construída: (para f = 1/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo hyperparâmetros\n",
    "num_epochs= 100\n",
    "Nn_hid_layer= 3\n",
    "eta= 0.5\n",
    "function = 'inverse'\n",
    "init = 'xavier'\n",
    "\n",
    "#Chamando funcção train_test e imprimindo resultados\n",
    "J_MSE_train, x_test, d_test, y_test, mse, abse = train_test_model(num_pts_train, num_pts_test, num_epochs, Nn_hid_layer, eta, init, function)\n",
    "print_results(J_MSE_train, x_test, d_test, y_test, num_epochs, Nn_hid_layer, eta, function, mse, abse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de uso da função construída (2) (para f = log10(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemplo com log\n",
    "#Definindo hyperparâmetros\n",
    "num_epochs= 100\n",
    "Nn_hid_layer= 3\n",
    "eta= 0.5\n",
    "function = 'log'\n",
    "init = 'xavier'\n",
    "\n",
    "#Chamando funcção train_test e imprimindo resultados\n",
    "J_MSE_train, x_test, d_test, y_test, mse, abse = train_test_model(num_pts_train, num_pts_test, num_epochs, Nn_hid_layer, eta, init, function)\n",
    "print_results(J_MSE_train, x_test, d_test, y_test, num_epochs, Nn_hid_layer, eta, function, mse, abse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo grid search para encontrar os melhores parâmetros"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo dicionário de grid-search dos hyperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {\n",
    "    'num_epochs': [1000, 5000],\n",
    "    'Nn_hid_layer': [15, 20, 50, 100],\n",
    "    'eta': [0.6, 1.2],\n",
    "}\n",
    "# grid_search = {\n",
    "#     'num_epochs': [1, 10],\n",
    "#     'Nn_hid_layer': [3, 4],\n",
    "#     'eta': [0.1,],\n",
    "# }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo grid search para inversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_min: 0.010570667553193172 \n",
      "melhores hyperparâmetros: [1000, 20, 0.6]\n",
      "mse_min: 45940.969896287956 \n",
      "melhores hyperparâmetros: [5000, 100, 0.6]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Extract the parameter values from the grid_search dictionary\n",
    "param_values = list(grid_search.values())\n",
    "function = 'inverse'\n",
    "init = 'xavier'\n",
    "mse_min = 100\n",
    "abse_min = 10**10\n",
    "# Iterate through all possible combinations of the parameters\n",
    "\n",
    "for num_epochs, Nn_hid_layer, eta in itertools.product(*param_values):\n",
    "    J_MSE_train, x_test, d_test, y_test, mse, abse = train_test_model(num_pts_train, num_pts_test, num_epochs, Nn_hid_layer, eta, init, function)\n",
    "    print_results(J_MSE_train, x_test, d_test, y_test, num_epochs, Nn_hid_layer, eta, function, mse, abse)\n",
    "    if abse < abse_min:\n",
    "        abse_min = abse\n",
    "        best_params_abse = [num_epochs, Nn_hid_layer, eta]\n",
    "    if mse < mse_min:\n",
    "        mse_min = mse\n",
    "        best_params_mse = [num_epochs, Nn_hid_layer, eta]\n",
    "print(f'mse_min: {mse_min} \\nmelhores hyperparâmetros: {best_params_mse}')\n",
    "print(f'mse_min: {abse_min} \\nmelhores hyperparâmetros: {best_params_abse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_min: 0.010570667553193172 \n",
      "melhores hyperparâmetros: [1000, 20, 0.6]\n",
      "mse_min: 45940.969896287956 \n",
      "melhores hyperparâmetros: [5000, 100, 0.6]\n"
     ]
    }
   ],
   "source": [
    "function = 'log'\n",
    "\n",
    "# Iterate through all possible combinations of the parameters\n",
    "for num_epochs, Nn_hid_layer, eta in itertools.product(*param_values):\n",
    "    J_MSE_train, x_test, d_test, y_test, mse, abse = train_test_model(num_pts_train, num_pts_test, num_epochs, Nn_hid_layer, eta, init, function)\n",
    "    print_results(J_MSE_train, x_test, d_test, y_test, num_epochs, Nn_hid_layer, eta, function, mse, abse)\n",
    "    if abse < abse_min:\n",
    "        abse_min = abse\n",
    "        best_params_abse = [num_epochs, Nn_hid_layer, eta]\n",
    "    if mse < mse_min:\n",
    "        mse_min = mse\n",
    "        best_params_mse = [num_epochs, Nn_hid_layer, eta]\n",
    "print(f'mse_min: {mse_min} \\nmelhores hyperparâmetros: {best_params_mse}')\n",
    "print(f'mse_min: {abse_min} \\nmelhores hyperparâmetros: {best_params_abse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_min: 0.007566691115223755 \n",
      "melhores hyperparâmetros: [1000, 15, 1.2]\n",
      "mse_min: 45940.969896287956 \n",
      "melhores hyperparâmetros: [5000, 100, 0.6]\n"
     ]
    }
   ],
   "source": [
    "function = 'exp'\n",
    "for num_epochs, Nn_hid_layer, eta in itertools.product(*param_values):\n",
    "    J_MSE_train, x_test, d_test, y_test, mse, abse = train_test_model(num_pts_train, num_pts_test, num_epochs, Nn_hid_layer, eta, init, function)\n",
    "    print_results(J_MSE_train, x_test, d_test, y_test, num_epochs, Nn_hid_layer, eta, function, mse, abse)\n",
    "    if abse < abse_min:\n",
    "        abse_min = abse\n",
    "        best_params_abse = [num_epochs, Nn_hid_layer, eta]\n",
    "    if mse < mse_min:\n",
    "        mse_min = mse\n",
    "        best_params_mse = [num_epochs, Nn_hid_layer, eta]\n",
    "print(f'mse_min: {mse_min} \\nmelhores hyperparâmetros: {best_params_mse}')\n",
    "print(f'mse_min: {abse_min} \\nmelhores hyperparâmetros: {best_params_abse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_min: 0.007566691115223755 \n",
      "melhores hyperparâmetros: [1000, 15, 1.2]\n",
      "mse_min: 45940.969896287956 \n",
      "melhores hyperparâmetros: [5000, 100, 0.6]\n"
     ]
    }
   ],
   "source": [
    "function = 'sin'\n",
    "for num_epochs, Nn_hid_layer, eta in itertools.product(*param_values):\n",
    "    J_MSE_train, x_test, d_test, y_test, mse, abse = train_test_model(num_pts_train, num_pts_test, num_epochs, Nn_hid_layer, eta, init, function)\n",
    "    print_results(J_MSE_train, x_test, d_test, y_test, num_epochs, Nn_hid_layer, eta, function, mse, abse)\n",
    "    if abse < abse_min:\n",
    "        abse_min = abse\n",
    "        best_params_abse = [num_epochs, Nn_hid_layer, eta]\n",
    "    if mse < mse_min:\n",
    "        mse_min = mse\n",
    "        best_params_mse = [num_epochs, Nn_hid_layer, eta]\n",
    "print(f'mse_min: {mse_min} \\nmelhores hyperparâmetros: {best_params_mse}')\n",
    "print(f'mse_min: {abse_min} \\nmelhores hyperparâmetros: {best_params_abse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visagio-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
